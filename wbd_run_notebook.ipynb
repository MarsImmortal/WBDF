{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## kdb utils (only when broad run seperately)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "from pyitlib import discrete_random_variable as drv\n",
    "from warnings import warn\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from tqdm import tqdm\n",
    "\n",
    "def build_graph(X_train, y_train, k=2):\n",
    "  '''\n",
    "  kDB algorithm\n",
    "\n",
    "  Param:\n",
    "  ----------------------\n",
    "    \n",
    "  Return:\n",
    "  ----------------------\n",
    "  graph edges\n",
    "  '''\n",
    "  #ensure data\n",
    "  num_features = X_train.shape[1]\n",
    "  x_nodes = list(range(num_features))\n",
    "  y_node  = num_features\n",
    "  #X_train = X_train.to_numpy()\n",
    "  #y_train = y_train.to_numpy()\n",
    "\n",
    "  #util func\n",
    "  _x = lambda i:X_train[:,i]\n",
    "  _x2comb = lambda i,j:(X_train[:,i], X_train[:,j])\n",
    "\n",
    "  #feature indexes desc sort by mutual information\n",
    "  sorted_feature_idxs = np.argsort([\n",
    "    drv.information_mutual(_x(i), y_train) \n",
    "    for i in range(num_features)\n",
    "  ])[::-1]\n",
    "\n",
    "  #start building graph\n",
    "  edges = []\n",
    "  for iter, target_idx in enumerate(sorted_feature_idxs):\n",
    "    target_node = x_nodes[target_idx]\n",
    "    edges.append((y_node, target_node))\n",
    "\n",
    "    parent_candidate_idxs = sorted_feature_idxs[:iter]\n",
    "    if iter <= k:\n",
    "      for idx in parent_candidate_idxs:\n",
    "        edges.append((x_nodes[idx], target_node))\n",
    "    else:\n",
    "      first_k_parent_mi_idxs = np.argsort([\n",
    "        drv.information_mutual_conditional(*_x2comb(i, target_idx), y_train)\n",
    "        for i in parent_candidate_idxs\n",
    "      ])[::-1][:k]\n",
    "      first_k_parent_idxs = parent_candidate_idxs[first_k_parent_mi_idxs]\n",
    "\n",
    "      for parent_idx in first_k_parent_idxs:\n",
    "        edges.append((x_nodes[parent_idx], target_node))\n",
    "  return edges\n",
    "\n",
    "def draw_graph(edges):\n",
    "  '''\n",
    "  Draw the graph\n",
    "\n",
    "  Param\n",
    "  -----------------\n",
    "  edges: edges of the graph\n",
    "\n",
    "  '''\n",
    "  graph = nx.DiGraph(edges)\n",
    "  pos=nx.spiral_layout(graph)\n",
    "  nx.draw(graph, pos, node_color='r', edge_color='b')\n",
    "  nx.draw_networkx_labels(graph, pos, font_size=20, font_family=\"sans-serif\")\n",
    "\n",
    "from tensorflow.keras.layers import Input,Embedding, Dense,Dropout, concatenate, Flatten\n",
    "from tensorflow.python.keras.regularizers import l1_l2, l2\n",
    "from tensorflow.keras.models import Model\n",
    "from pandas import DataFrame, Series, crosstab\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "def get_cross_table(*cols, apply_wt=False):\n",
    "    '''   \n",
    "    author: alexland\n",
    "\n",
    "    returns:\n",
    "      (i) xt, NumPy array storing the xtab results, number of dimensions is equal to \n",
    "          the len(args) passed in\n",
    "      (ii) unique_vals_all_cols, a tuple of 1D NumPy array for each dimension \n",
    "          in xt (for a 2D xtab, the tuple comprises the row and column headers)\n",
    "      pass in:\n",
    "        (i) 1 or more 1D NumPy arrays of integers\n",
    "        (ii) if wts is True, then the last array in cols is an array of weights\n",
    "        \n",
    "    if return_inverse=True, then np.unique also returns an integer index \n",
    "    (from 0, & of same len as array passed in) such that, uniq_vals[idx] gives the original array passed in\n",
    "    higher dimensional cross tabulations are supported (eg, 2D & 3D)\n",
    "    cross tabulation on two variables (columns):\n",
    "    >>> q1 = np.array([7, 8, 8, 8, 5, 6, 4, 6, 6, 8, 4, 6, 6, 6, 6, 8, 8, 5, 8, 6])\n",
    "    >>> q2 = np.array([6, 4, 6, 4, 8, 8, 4, 8, 7, 4, 4, 8, 8, 7, 5, 4, 8, 4, 4, 4])\n",
    "    >>> uv, xt = xtab(q1, q2)\n",
    "    >>> uv\n",
    "      (array([4, 5, 6, 7, 8]), array([4, 5, 6, 7, 8]))\n",
    "    >>> xt\n",
    "      array([[2, 0, 0, 0, 0],\n",
    "             [1, 0, 0, 0, 1],\n",
    "             [1, 1, 0, 2, 4],\n",
    "             [0, 0, 1, 0, 0],\n",
    "             [5, 0, 1, 0, 1]], dtype=uint64)\n",
    "      '''\n",
    "    if not all(len(col) == len(cols[0]) for col in cols[1:]):\n",
    "      raise ValueError(\"all arguments must be same size\")\n",
    "\n",
    "    if len(cols) == 0:\n",
    "      raise TypeError(\"xtab() requires at least one argument\")\n",
    "\n",
    "    fnx1 = lambda q: len(q.squeeze().shape)\n",
    "    if not all([fnx1(col) == 1 for col in cols]):\n",
    "      raise ValueError(\"all input arrays must be 1D\")\n",
    "\n",
    "    if apply_wt:\n",
    "      cols, wt = cols[:-1], cols[-1]\n",
    "    else:\n",
    "      wt = 1\n",
    "\n",
    "    uniq_vals_all_cols, idx = zip( *(np.unique(col, return_inverse=True) for col in cols) )\n",
    "    shape_xt = [uniq_vals_col.size for uniq_vals_col in uniq_vals_all_cols]\n",
    "    dtype_xt = 'float' if apply_wt else 'uint'\n",
    "    xt = np.zeros(shape_xt, dtype=dtype_xt)\n",
    "    np.add.at(xt, idx, wt)\n",
    "    return uniq_vals_all_cols, xt\n",
    "\n",
    "def _get_dependencies_without_y(variables, y_name, kdb_edges):\n",
    "    ''' \n",
    "    evidences of each variable without y.\n",
    "\n",
    "    Param:\n",
    "    --------------\n",
    "    variables: variable names\n",
    "\n",
    "    y_name: class name\n",
    "\n",
    "    kdb_edges: list of tuple (source, target)\n",
    "    '''\n",
    "    dependencies = {}\n",
    "    kdb_edges_without_y = [edge for edge in kdb_edges if edge[0] != y_name]\n",
    "    mi_desc_order = {t:i for i,(s,t) in enumerate(kdb_edges) if s == y_name}\n",
    "    for x in variables:\n",
    "        current_dependencies = [s for s,t in kdb_edges_without_y if t == x]\n",
    "        if len(current_dependencies) >= 2:\n",
    "            sort_dict = {t:mi_desc_order[t] for t in current_dependencies}        \n",
    "            dependencies[x] = sorted(sort_dict)\n",
    "        else:\n",
    "            dependencies[x] = current_dependencies\n",
    "    return dependencies\n",
    "\n",
    "def _add_uniform(array, noise=1e-5):\n",
    "    ''' \n",
    "    if no count on particular condition for any feature, give a uniform prob rather than leave 0\n",
    "    '''\n",
    "    sum_by_col = np.sum(array,axis=0)\n",
    "    zero_idxs = (array == 0).astype(int)\n",
    "    # zero_count_by_col = np.sum(zero_idxs,axis=0)\n",
    "    nunique = array.shape[0]\n",
    "    result = np.zeros_like(array, dtype='float')\n",
    "    for i in range(array.shape[1]):\n",
    "        if sum_by_col[i] == 0:\n",
    "            result[:,i] = array[:,i] + 1./nunique\n",
    "        elif noise != 0:\n",
    "            result[:,i] = array[:,i] + noise * zero_idxs[:,i]\n",
    "        else:\n",
    "            result[:,i] = array[:,i]\n",
    "    return result\n",
    "\n",
    "def get_kdb_embeddings(X_train, y_train, k=2, noise=1e-7, dtype='float32'):\n",
    "    #assert(isinstance(X_train, DataFrame))\n",
    "    #assert(isinstance(y_train, Series))\n",
    "\n",
    "    kdb_embeddings = []\n",
    "\n",
    "    # dependency graph generated by kdb algorithm\n",
    "    edges = build_graph(X_train, y_train, k)\n",
    "    n_classes = len(np.unique(y_train))\n",
    "    num_features = X_train.shape[1]\n",
    "    if k > 0:\n",
    "        dependencies = _get_dependencies_without_y(list(range(num_features)), num_features, edges)\n",
    "    else:\n",
    "        dependencies = {x:[] for x in range(num_features)}\n",
    "\n",
    "    for x, evidences in dependencies.items():\n",
    "        evidences = [X_train[:,e] for e in evidences] + [y_train]\n",
    "        #evidences = np.hstack([X_train, y_train.reshape(-1,1)])\n",
    "        # conditional probalility table of x\n",
    "        normalized_cct = crosstab(X_train[:,x], evidences, dropna=False,normalize='columns').to_numpy()\n",
    "        normalized_cct = _add_uniform(normalized_cct, noise)\n",
    "        current_embeddings = np.log2(normalized_cct, dtype=dtype)\n",
    "        kdb_embeddings.append(current_embeddings)\n",
    "    \n",
    "    return kdb_embeddings\n",
    "\n",
    "def get_high_order_feature(X, col, evidence_cols, feature_uniques):\n",
    "    '''\n",
    "    encode the high order feature of X[col] given evidences X[evidence_cols].\n",
    "    '''\n",
    "    if evidence_cols is None or len(evidence_cols) == 0:\n",
    "        return X[:,[col]]\n",
    "    else:\n",
    "        evidences = [X[:,_col] for _col in evidence_cols]\n",
    "\n",
    "        #[1, variable_unique, evidence_unique]\n",
    "        base = [1, feature_uniques[col]] + [feature_uniques[_col] for _col in evidence_cols[::-1][:-1]]\n",
    "        cum_base = np.cumprod(base)[::-1]\n",
    "        \n",
    "        cols = evidence_cols + [col]\n",
    "        high_order_feature = np.sum(X[:,cols] * cum_base, axis=1).reshape(-1,1)\n",
    "        return high_order_feature\n",
    "\n",
    "def get_high_order_constraints(X, col, evidence_cols, feature_uniques):\n",
    "    '''\n",
    "    find the constraints infomation for the high order feature X[col] given evidences X[evidence_cols].\n",
    "    \n",
    "    Returns:\n",
    "    ---------------------\n",
    "    tuple(have_value, high_order_uniques)\n",
    "\n",
    "    have_value: a k+1 dimensions numpy ndarray of type boolean. \n",
    "        Each dimension correspond to a variable, with the order (*evidence_cols, col)\n",
    "        True indicate the corresponding combination of variable values cound be found in the dataset.\n",
    "        False indicate not.\n",
    "\n",
    "    high_order_constraints: a 1d nummy ndarray of type int.\n",
    "        Each number `c` indicate that there are `c` cols shound be applying the constraints since the last constrant position(or index 0),\n",
    "        in sequence.         \n",
    "\n",
    "    '''\n",
    "    if evidence_cols is None or len(evidence_cols) == 0:\n",
    "        unique = feature_uniques[col]\n",
    "        return np.ones(unique,dtype=bool), np.array([unique])\n",
    "    else:\n",
    "        cols = evidence_cols + [col]\n",
    "        cross_table_idxs, cross_table = get_cross_table(*[X[:,i] for i in cols])\n",
    "        have_value = cross_table != 0\n",
    "    \n",
    "        have_value_reshape = have_value.reshape(-1,have_value.shape[-1])\n",
    "        have_value_split = np.split(have_value_reshape, have_value_reshape.shape[0], 0)\n",
    "        high_order_constraints = np.sum(have_value_reshape, axis=-1)\n",
    "    \n",
    "        return have_value, high_order_constraints\n",
    "\n",
    "class KdbHighOrderFeatureEncoder:\n",
    "    '''\n",
    "    build a kdb model, use the dependency relationships to encode high order feature of given dataset.\n",
    "    '''\n",
    "    def __init__(self):\n",
    "        self.dependencies_ = {}\n",
    "        self.constraints_ = np.array([])\n",
    "        self.have_value_idxs_ = []\n",
    "        self.feature_uniques_ = []\n",
    "        self.high_order_feature_uniques_ = []\n",
    "        self.edges_ = []\n",
    "        self.ohe = None\n",
    "    \n",
    "    def fit(self, X_train, y_train, k=2):\n",
    "        '''\n",
    "        build the kdb model, obtain the dependencies.\n",
    "        '''\n",
    "        edges = build_graph(X_train, y_train, k)\n",
    "        n_classes = len(np.unique(y_train))\n",
    "        num_features = X_train.shape[1]\n",
    "\n",
    "        if k > 0:\n",
    "            dependencies = _get_dependencies_without_y(list(range(num_features)), num_features, edges)\n",
    "        else:\n",
    "            dependencies = {x:[] for x in range(num_features)}\n",
    "        \n",
    "        self.dependencies_ = dependencies\n",
    "        self.feature_uniques_ = [len(np.unique(X_train[:,i])) for i in range(num_features)]\n",
    "        self.edges_ = edges\n",
    "        return self\n",
    "        \n",
    "    def transform(self, X, return_constraints=True):\n",
    "        '''\n",
    "        encode the high order feature,find corresbonding constraints info,\n",
    "        then arrange to a proper format and return(store) it.\n",
    "        '''\n",
    "        high_order_features = []\n",
    "        have_value_idxs = []\n",
    "        constraints = []\n",
    "        for k, v in self.dependencies_.items():\n",
    "            hio = get_high_order_feature(X, k, v, self.feature_uniques_)\n",
    "            idx, constraint = get_high_order_constraints(X, k, v, self.feature_uniques_)\n",
    "        \n",
    "            high_order_features.append(hio)\n",
    "            have_value_idxs.append(idx)\n",
    "            constraints.append(constraint)\n",
    "        \n",
    "        concated_constraints = np.hstack(constraints)\n",
    "        concated_high_order_features = np.hstack(high_order_features)\n",
    "        \n",
    "        from sklearn.preprocessing import OneHotEncoder\n",
    "        if self.ohe is None:\n",
    "            self.ohe = OneHotEncoder()\n",
    "            self.ohe.fit(concated_high_order_features)\n",
    "        X_high_order = self.ohe.transform(concated_high_order_features)\n",
    "    \n",
    "        self.high_order_feature_uniques_ = [np.sum(constraint) for constraint in constraints]\n",
    "        self.constraints_ = concated_constraints\n",
    "        self.have_value_idxs_ = have_value_idxs\n",
    "\n",
    "        if return_constraints:\n",
    "            return X_high_order, concated_constraints, have_value_idxs\n",
    "        else:\n",
    "            return X_high_order\n",
    "    \n",
    "    def fit_transform(self, X, y, k=2, return_constraints=True):\n",
    "        return self.fit(X, y, k).transform(X, return_constraints)\n",
    "\n",
    "def sample_synthetic_data(weights, kdb_high_order_encoder, y_counts, ohe=True,size=None):\n",
    "    from pgmpy.models import BayesianModel\n",
    "    from pgmpy.sampling import BayesianModelSampling\n",
    "    from pgmpy.factors.discrete import TabularCPD\n",
    "    #basic varibles\n",
    "    feature_cards = np.array(kdb_high_order_encoder.feature_uniques_)\n",
    "    n_features = len(feature_cards)\n",
    "    n_classes = weights.shape[1]\n",
    "    n_samples = y_counts.sum()\n",
    "\n",
    "    #ensure sum of each constraint group equals to 1, then re concat the probs\n",
    "    _idxs = np.cumsum([0] + kdb_high_order_encoder.constraints_.tolist())\n",
    "    constraint_idxs = [(_idxs[i],_idxs[i+1]) for i in range(len(_idxs)-1)]\n",
    "    \n",
    "    probs = np.exp(weights)\n",
    "    cpd_probs = [probs[start:end,:] for start, end in constraint_idxs]\n",
    "    cpd_probs = np.vstack([p/p.sum(axis=0) for p in cpd_probs])\n",
    "\n",
    "    #assign the probs to the full cpd tables\n",
    "    idxs = np.cumsum([0] + kdb_high_order_encoder.high_order_feature_uniques_)\n",
    "    feature_idxs = [(idxs[i],idxs[i+1]) for i in range(len(idxs)-1)]\n",
    "    have_value_idxs = kdb_high_order_encoder.have_value_idxs_\n",
    "    full_cpd_probs = [] \n",
    "    for have_value, (start, end) in zip(have_value_idxs, feature_idxs):\n",
    "        #(n_high_order_feature_uniques, n_classes)\n",
    "        cpd_prob_ = cpd_probs[start:end,:]\n",
    "        #(n_all_combination) Note: the order is (*parent, variable)\n",
    "        have_value_ravel = have_value.ravel()\n",
    "        #(n_classes * n_all_combination)\n",
    "        have_value_ravel_repeat = np.hstack([have_value_ravel] * n_classes)\n",
    "        #(n_classes * n_all_combination) <- (n_classes * n_high_order_feature_uniques)\n",
    "        full_cpd_prob_ravel = np.zeros_like(have_value_ravel_repeat, dtype=float)\n",
    "        full_cpd_prob_ravel[have_value_ravel_repeat] = cpd_prob_.T.ravel()\n",
    "        #(n_classes * n_parent_combinations, n_variable_unique)\n",
    "        full_cpd_prob = full_cpd_prob_ravel.reshape(-1, have_value.shape[-1]).T\n",
    "        full_cpd_prob = _add_uniform(full_cpd_prob, noise=0)\n",
    "        full_cpd_probs.append(full_cpd_prob)\n",
    "\n",
    "    #prepare node and edge names\n",
    "    node_names = [str(i) for i in range(n_features + 1)]\n",
    "    edge_names = [(str(i), str(j)) for i,j in kdb_high_order_encoder.edges_]\n",
    "    y_name = node_names[-1]\n",
    "\n",
    "    #create TabularCPD objects\n",
    "    evidences = kdb_high_order_encoder.dependencies_\n",
    "    feature_cpds = [\n",
    "        TabularCPD(str(name), feature_cards[name], table, \n",
    "                   evidence=[y_name, *[str(e) for e in evidences]], \n",
    "                   evidence_card=[n_classes, *feature_cards[evidences].tolist()])\n",
    "        for (name, evidences), table in zip(evidences.items(), full_cpd_probs)\n",
    "    ]\n",
    "    y_probs = (y_counts/n_samples).reshape(-1,1)\n",
    "    y_cpd = TabularCPD(y_name, n_classes, y_probs)\n",
    "\n",
    "    #create kDB model, then sample data\n",
    "    model = BayesianModel(edge_names)\n",
    "    model.add_cpds(y_cpd, *feature_cpds)\n",
    "    sample_size = n_samples if size is None else size\n",
    "    result = BayesianModelSampling(model).forward_sample(size=sample_size)\n",
    "    sorted_result = result[node_names].values\n",
    "\n",
    "    #return\n",
    "    syn_X, syn_y = sorted_result[:,:-1], sorted_result[:,-1]\n",
    "    if ohe:\n",
    "        from sklearn.preprocessing import OneHotEncoder\n",
    "        ohe_syn_X = OneHotEncoder().fit_transform(syn_X)\n",
    "        return ohe_syn_X, syn_y\n",
    "    else:\n",
    "        return syn_X, syn_y    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.python.ops import math_ops, array_ops\n",
    "from tensorflow.keras.constraints import Constraint\n",
    "from tensorflow.keras.activations import softmax\n",
    "from tensorflow.keras.layers import Dense\n",
    "# Tensowrflow version 2.8, keras version 2.8, pyitlib 0.2.2 \n",
    "# Only when tensorflow version has any bug (incompatible), try below\n",
    "# from tensorflow.python.ops import math_ops, array_ops\n",
    "# from tensorflow.python.keras.constraints import Constraint\n",
    "# from tensorflow.python.keras.activations import softmax\n",
    "# from tensorflow.python.keras.layers import Dense\n",
    "\n",
    "class softmax_weight(Constraint):\n",
    "    \"\"\"Constrains weight tensors to be under softmax `.\"\"\"\n",
    "  \n",
    "    def __init__(self,feature_uniques):\n",
    "        idxs = math_ops.cumsum([0] + feature_uniques)\n",
    "        idxs = [i.numpy() for i in idxs]\n",
    "        self.feature_idxs = [\n",
    "            (idxs[i],idxs[i+1]) for i in range(len(idxs)-1)\n",
    "        ]\n",
    "  \n",
    "    def __call__(self, w):     \n",
    "        w_new = [\n",
    "            math_ops.log(softmax(w[i:j,:], axis=0))\n",
    "            for i,j in self.feature_idxs\n",
    "        ]\n",
    "        return tf.concat(w_new, 0)\n",
    "  \n",
    "    def get_config(self):\n",
    "        return {'feature_idxs': self.feature_idxs}\n",
    "    \n",
    "def broad(input_dim, output_dim, constraint):    \n",
    "    return Dense(output_dim, input_dim=input_dim, activation='softmax',kernel_constraint=constraint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define WBD(F)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepctr.layers.core import PredictionLayer, DNN\n",
    "from deepctr.layers.interaction import CIN\n",
    "from deepctr.layers.utils import concat_func, add_func, combined_dnn_input\n",
    "from deepctr.feature_column import build_input_features, get_linear_logit, input_from_feature_columns\n",
    "from deepctr.models import xDeepFM\n",
    "from tensorflow.python.keras.layers import Input\n",
    "from tensorflow.python.keras.models import Model\n",
    "def wbdf(elr_constrains, broad_units, dnn_feature_columns,output_units, dnn_hidden_units=(256, 256),\n",
    "            cin_layer_size=(128, 128,), cin_split_half=True, cin_activation='relu', l2_reg_linear=0.00001,\n",
    "            use_fm=False, fm_group=None,\n",
    "            l2_reg_embedding=0.00001, l2_reg_dnn=0, l2_reg_cin=0, seed=1024, dnn_dropout=0,\n",
    "            dnn_activation='relu', dnn_use_bn=False, task='binary'):\n",
    "    \"\"\"\tInstantiates the wbdf architecture.\n",
    "\n",
    "    :param linear_feature_columns: An iterable containing all the features used by linear part of the model.\n",
    "    :param dnn_feature_columns: An iterable containing all the features used by deep part of the model.\n",
    "    :param dnn_hidden_units: list,list of positive integer or empty list, the layer number and units in each layer of deep net\n",
    "    :param cin_layer_size: list,list of positive integer or empty list, the feature maps  in each hidden layer of Compressed Interaction Network\n",
    "    :param cin_split_half: bool.if set to True, half of the feature maps in each hidden will connect to output unit\n",
    "    :param cin_activation: activation function used on feature maps\n",
    "    :param l2_reg_linear: float. L2 regularizer strength applied to linear part\n",
    "    :param l2_reg_embedding: L2 regularizer strength applied to embedding vector\n",
    "    :param l2_reg_dnn: L2 regularizer strength applied to deep net\n",
    "    :param l2_reg_cin: L2 regularizer strength applied to CIN.\n",
    "    :param seed: integer ,to use as random seed.\n",
    "    :param dnn_dropout: float in [0,1), the probability we will drop out a given DNN coordinate.\n",
    "    :param dnn_activation: Activation function to use in DNN\n",
    "    :param dnn_use_bn: bool. Whether use BatchNormalization before activation or not in DNN\n",
    "    :param task: str, ``\"binary\"`` for  binary logloss or  ``\"regression\"`` for regression loss\n",
    "    :return: A Keras model instance.\n",
    "    \"\"\"\t\n",
    "\n",
    "    features = build_input_features(dnn_feature_columns)\n",
    "    broad_inputs = Input((broad_units,), name='broad')\n",
    "    \n",
    "    inputs_list = list(features.values())\n",
    "    inputs_list.insert(0, broad_inputs)\n",
    "    \n",
    "    widebroad_output = broad(broad_units, output_units, elr_constrains)(broad_inputs)\n",
    "    \n",
    "    \n",
    "    sparse_embedding_list, dense_value_list = input_from_feature_columns(features, dnn_feature_columns,\n",
    "                                                                         l2_reg_embedding, seed)\n",
    "    cin_input = concat_func(sparse_embedding_list, axis=1)\n",
    "\n",
    "    dnn_input = combined_dnn_input(sparse_embedding_list, dense_value_list)\n",
    "    dnn_output = DNN(dnn_hidden_units, dnn_activation, l2_reg_dnn, dnn_dropout, dnn_use_bn, seed=seed)(dnn_input)\n",
    "    dnn_output = tf.keras.layers.Dense(\n",
    "        output_units, use_bias=False, kernel_initializer=tf.keras.initializers.glorot_normal(seed))(dnn_output)\n",
    "    # here, you could link any other component for future, such as transformer \n",
    "    output = tf.concat([widebroad_output, dnn_output], axis=-1)\n",
    "    # here, if you want to use cin or others\n",
    "    if len(cin_layer_size) > 0:\n",
    "        exFM_out = CIN(cin_layer_size, cin_activation,\n",
    "                       cin_split_half, l2_reg_cin, seed)(cin_input)\n",
    "        exFM_logit = tf.keras.layers.Dense(output_units, kernel_initializer=tf.keras.initializers.glorot_normal(seed))(exFM_out)\n",
    "        output = tf.concat([output, exFM_logit], axis=-1)\n",
    "    \n",
    "    #here, you could adjust for multiple class or binary/regression \n",
    "    output = DNN([100,50], dnn_activation, l2_reg_dnn, dnn_dropout, dnn_use_bn, seed=seed)(output)\n",
    "    output = tf.keras.layers.Dense(\n",
    "        1, use_bias=False, kernel_initializer=tf.keras.initializers.glorot_normal(seed))(dnn_output)\n",
    "    output = PredictionLayer(task)(output)\n",
    "\n",
    "    model = Model(inputs=inputs_list, outputs=output)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "#data = pd.read_csv('../../uci-datasets/mdl/adult.csv')\n",
    "data = pd.read_csv('adult-dm.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "age               int64\n",
       "workclass         int64\n",
       "fnlwgt            int64\n",
       "education         int64\n",
       "education-num     int64\n",
       "marital-status    int64\n",
       "occupation        int64\n",
       "relationship      int64\n",
       "race              int64\n",
       "sex               int64\n",
       "capital-gain      int64\n",
       "capital-loss      int64\n",
       "hours-per-week    int64\n",
       "native-country    int64\n",
       "class             int64\n",
       "dtype: object"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# if use broad, must make sure the input is int32\n",
    "data.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from deepctr.models import DeepFM\n",
    "from deepctr.feature_column import SparseFeat, DenseFeat,get_feature_names\n",
    "\n",
    "sparse_features = data.columns[:-1]\n",
    "dense_features = []\n",
    "\n",
    "data[sparse_features] = data[sparse_features].fillna('-1', )\n",
    "#data[dense_features] = data[dense_features].fillna(0,)\n",
    "target = data.columns[-1]\n",
    "\n",
    "for feat in sparse_features:\n",
    "    lbe = LabelEncoder()\n",
    "    data[feat] = lbe.fit_transform(data[feat])\n",
    "\n",
    "data[target] = LabelEncoder().fit_transform(data[target])\n",
    "    \n",
    "#mms = MinMaxScaler(feature_range=(0,1))\n",
    "#data[dense_features] = mms.fit_transform(data[dense_features])\n",
    "\n",
    "fixlen_feature_columns = [SparseFeat(feat, vocabulary_size=data[feat].max() + 1,embedding_dim=4)\n",
    "                            for i,feat in enumerate(sparse_features)] \n",
    "                        # + [DenseFeat(feat, 1,) for feat in dense_features]\n",
    "                       \n",
    "dnn_feature_columns = fixlen_feature_columns\n",
    "linear_feature_columns = fixlen_feature_columns\n",
    "\n",
    "feature_names = get_feature_names(linear_feature_columns + dnn_feature_columns)\n",
    "\n",
    "train, test = train_test_split(data, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_model_input = {name:train[name].values for name in feature_names}\n",
    "test_model_input = {name:test[name].values for name in feature_names}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WBD start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ganblr.kdb import KdbHighOrderFeatureEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
    "enc = KdbHighOrderFeatureEncoder()\n",
    "\n",
    "# X_highorder_train = enc.fit_transform(train.iloc[:,:-1].values, train[target].values, k=2, return_constraints=False)\n",
    "# X_highorder_test = enc.transform(test.iloc[:,:-1].values, False)\n",
    "# generate broad encoders based on the k order\n",
    "all = pd.concat([train,test],axis=0)\n",
    "X_highorder = enc.fit_transform(all.iloc[:,:-1].values.astype('int'),  all[target].values.astype('int'), k=2, return_constraints=False)\n",
    "X_highorder_train = X_highorder[:train.shape[0]]\n",
    "X_highorder_test = X_highorder[train.shape[0]:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_highorder_train_np = X_highorder_train.toarray()\n",
    "X_highorder_test_np = X_highorder_test.toarray()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if you want to create seperate ohe feature, you could do it here\n",
    "# ohe = OneHotEncoder(handle_unknown='ignore')\n",
    "# X_ohe_train = ohe.fit_transform(train.values[:,:-1]).toarray()\n",
    "# X_ohe_test = ohe.transform(test.values[:,:-1]).toarray()\n",
    "# define target for train and test\n",
    "y_train = train.values[:,[-1]]\n",
    "y_test = test.values[:,[-1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import copy\n",
    "train_dict = copy(train_model_input)\n",
    "train_dict['broad'] = X_highorder_train_np\n",
    "\n",
    "test_dict = copy(test_model_input)\n",
    "test_dict['broad'] = X_highorder_test_np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.linalg.matmul_9), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'dense_9/kernel:0' shape=(5531, 2) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.nn.bias_add_3), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'dense_9/bias:0' shape=(2,) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.linalg.matmul_10), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'dense_10/kernel:0' shape=(256, 2) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.linalg.matmul_11), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'dense_11/kernel:0' shape=(2, 1) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n"
     ]
    }
   ],
   "source": [
    "wbd = wbdf(softmax_weight(enc.constraints_), X_highorder_train.shape[1], dnn_feature_columns,2, cin_layer_size=[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "wbd.compile(\"adam\", \"binary_crossentropy\",\n",
    "              metrics=['accuracy'], )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "245/245 [==============================] - 2s 4ms/step - loss: 0.3313 - accuracy: 0.8512 - val_loss: 0.2914 - val_accuracy: 0.8655\n",
      "Epoch 2/20\n",
      "245/245 [==============================] - 1s 2ms/step - loss: 0.2764 - accuracy: 0.8744 - val_loss: 0.2872 - val_accuracy: 0.8678\n",
      "Epoch 3/20\n",
      "245/245 [==============================] - 1s 2ms/step - loss: 0.2737 - accuracy: 0.8750 - val_loss: 0.2844 - val_accuracy: 0.8686\n",
      "Epoch 4/20\n",
      "245/245 [==============================] - 1s 2ms/step - loss: 0.2721 - accuracy: 0.8749 - val_loss: 0.2824 - val_accuracy: 0.8697\n",
      "Epoch 5/20\n",
      "245/245 [==============================] - 1s 2ms/step - loss: 0.2712 - accuracy: 0.8758 - val_loss: 0.2826 - val_accuracy: 0.8694\n",
      "Epoch 6/20\n",
      "245/245 [==============================] - 1s 2ms/step - loss: 0.2699 - accuracy: 0.8760 - val_loss: 0.2817 - val_accuracy: 0.8711\n",
      "Epoch 7/20\n",
      "245/245 [==============================] - 1s 2ms/step - loss: 0.2701 - accuracy: 0.8760 - val_loss: 0.2820 - val_accuracy: 0.8694\n",
      "Epoch 8/20\n",
      "245/245 [==============================] - 1s 2ms/step - loss: 0.2678 - accuracy: 0.8769 - val_loss: 0.2837 - val_accuracy: 0.8713\n",
      "Epoch 9/20\n",
      "245/245 [==============================] - 1s 2ms/step - loss: 0.2672 - accuracy: 0.8772 - val_loss: 0.2832 - val_accuracy: 0.8686\n",
      "Epoch 10/20\n",
      "245/245 [==============================] - 1s 2ms/step - loss: 0.2662 - accuracy: 0.8779 - val_loss: 0.2798 - val_accuracy: 0.8710\n",
      "Epoch 11/20\n",
      "245/245 [==============================] - 1s 2ms/step - loss: 0.2664 - accuracy: 0.8777 - val_loss: 0.2835 - val_accuracy: 0.8678\n",
      "Epoch 12/20\n",
      "245/245 [==============================] - 1s 3ms/step - loss: 0.2649 - accuracy: 0.8781 - val_loss: 0.2809 - val_accuracy: 0.8708\n",
      "Epoch 13/20\n",
      "245/245 [==============================] - 1s 2ms/step - loss: 0.2635 - accuracy: 0.8785 - val_loss: 0.2822 - val_accuracy: 0.8696\n",
      "Epoch 14/20\n",
      "245/245 [==============================] - 1s 2ms/step - loss: 0.2632 - accuracy: 0.8796 - val_loss: 0.2858 - val_accuracy: 0.8668\n",
      "Epoch 15/20\n",
      "245/245 [==============================] - 1s 2ms/step - loss: 0.2613 - accuracy: 0.8799 - val_loss: 0.2844 - val_accuracy: 0.8702\n",
      "Epoch 16/20\n",
      "245/245 [==============================] - 1s 2ms/step - loss: 0.2598 - accuracy: 0.8798 - val_loss: 0.2828 - val_accuracy: 0.8702\n",
      "Epoch 17/20\n",
      "245/245 [==============================] - 1s 2ms/step - loss: 0.2590 - accuracy: 0.8815 - val_loss: 0.2848 - val_accuracy: 0.8676\n",
      "Epoch 18/20\n",
      "245/245 [==============================] - 1s 2ms/step - loss: 0.2584 - accuracy: 0.8812 - val_loss: 0.2856 - val_accuracy: 0.8686\n",
      "Epoch 19/20\n",
      "245/245 [==============================] - 1s 2ms/step - loss: 0.2558 - accuracy: 0.8822 - val_loss: 0.2866 - val_accuracy: 0.8697\n",
      "Epoch 20/20\n",
      "245/245 [==============================] - 1s 2ms/step - loss: 0.2555 - accuracy: 0.8829 - val_loss: 0.2886 - val_accuracy: 0.8669\n"
     ]
    }
   ],
   "source": [
    "wbd_history = wbd.fit(train_dict, y_train, batch_size=128, epochs=20,validation_split=0.20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "77/77 [==============================] - 0s 1ms/step - loss: 0.2757 - accuracy: 0.8683\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.2756926417350769, 0.8682567477226257]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wbd.evaluate(test_dict, test[[target]].values, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "feb712b7fd78ffc8a45d6d802d5b3eee4b583c30e04419da5fcda15e904929c6"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
